import React, { useState, useEffect, useRef } from 'react';
import Header from './components/Header';
import InputBar from './components/InputBar';
import MessageBubble from './components/MessageBubble';
import { Message } from './types';
import { INITIAL_MESSAGE } from './constants';
import { analyzeImage } from './services/visionClient';
import { chatWithDeepseekStream } from './services/chatClient';
import { generateDesignImage, uploadImage } from './services/generateClient';
import { compressImage } from './services/utils';

// Helper: Parse render intent from user message
function hasRenderIntent(text: string): boolean {
  const keywords = ['æ•ˆæœåœ–', 'æ•ˆæœå›¾', 'å‡ºåœ–', 'å‡ºå›¾', 'æ¸²æŸ“', 'è¨­è¨ˆåœ–', 'è®¾è®¡å›¾', '3dåœ–', '3då›¾', 'æƒ³ç‡ä¸‹', 'æƒ³çœ‹ä¸€ä¸‹'];
  return keywords.some(k => text.includes(k));
}

const App: React.FC = () => {
  // Single chat history state
  const [messages, setMessages] = useState<Message[]>([INITIAL_MESSAGE]);
  const chatEndRef = useRef<HTMLDivElement>(null);
  
  // Pending image for vision analysis
  const [pendingImage, setPendingImage] = useState<{dataUrl: string, blobUrl?: string} | null>(null);
  
  // Render Intake State (Chat-based Flow)
  const [renderState, setRenderState] = useState<{
    isActive: boolean;
    step: 'space' | 'style' | 'color' | 'requirements' | 'ready';
    data: {
      space?: string;
      style?: string;
      color?: string;
      requirements?: string;
    };
    baseImageBlobUrl?: string; // Original photo
    lastGeneratedImageUrl?: string; // For "re-edit" flow
  }>({
    isActive: false,
    step: 'space',
    data: {}
  });

  const [generating, setGenerating] = useState(false);

  // Scroll to bottom
  const scrollToBottom = () => {
    chatEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  };

  useEffect(() => {
    scrollToBottom();
  }, [messages]);

  // Handle User Text Input
  const handleSendMessage = async (text: string) => {
    // 1. Add User Message
    const userMsg: Message = {
      id: Date.now().toString(),
      type: 'text',
      content: text,
      sender: 'user',
      timestamp: Date.now()
    };
    setMessages(prev => [...prev, userMsg]);

    // 2. Check for "Re-edit" Intent (if we have a last generated image)
    if (renderState.lastGeneratedImageUrl && (text.includes('å†æ”¹') || text.includes('ä¿®æ”¹') || text.includes('å””ä¿‚å¥½') || text.includes('ä¸å¦‚'))) {
        setGenerating(true);
        triggerImageGeneration(renderState.data, renderState.baseImageBlobUrl, renderState.lastGeneratedImageUrl, text);
        return;
    }

    // 3. Render Intake Flow (State Machine)
    if (renderState.isActive && renderState.step !== 'ready') {
      processRenderIntake(text);
      return;
    }

    // 4. Trigger Render Intake (if keywords found & image exists)
    if (hasRenderIntent(text)) {
      // Must have uploaded an image first to establish "scene context"
      // Or we check if there is any image in history?
      // For simplicity, let's check if we have a valid baseImage in state (from previous uploads)
      // or we ask user to upload one.
      
      const lastImageMsg = messages.slice().reverse().find(m => m.type === 'image');
      const baseBlobUrl = renderState.baseImageBlobUrl || (lastImageMsg?.content?.startsWith('http') ? lastImageMsg.content : undefined);

      if (!baseBlobUrl && !pendingImage) {
         const reply: Message = {
           id: Date.now().toString() + 'r',
           type: 'text',
           content: 'æƒ³å‡ºæ•ˆæœåœ–ç„¡å•é¡Œï¼éº»ç…©ä½ ä¸Šè¼‰ä¸€å¼µç¾å ´ç›¸ç‰‡å…ˆï¼Œç­‰æˆ‘å¯ä»¥è·Ÿè¿”å¯¦éš›çµæ§‹å»è¨­è¨ˆã€‚ğŸ“¸',
           sender: 'ai',
           timestamp: Date.now()
         };
         setMessages(prev => [...prev, reply]);
         return;
      }

      // Start Intake
      setRenderState(prev => ({
        ...prev,
        isActive: true,
        step: 'space',
        baseImageBlobUrl: baseBlobUrl || prev.baseImageBlobUrl
      }));
      
      const reply: Message = {
        id: Date.now().toString() + 'r',
        type: 'text',
        content: 'æ”¶åˆ°ï¼æƒ³å¹«ä½ å‡ºå¼µæ•ˆæœåœ–ã€‚é¦–å…ˆç¢ºèªä¸‹ï¼Œå‘¢å€‹ä¿‚é‚Šå€‹ç©ºé–“ï¼Ÿï¼ˆä¾‹å¦‚ï¼šå®¢å»³ã€ç¡æˆ¿ã€å»šæˆ¿â€¦ï¼‰',
        sender: 'ai',
        timestamp: Date.now(),
        options: ['å®¢å»³', 'é£¯å»³', 'ä¸»äººæˆ¿', 'ç¡æˆ¿', 'å»šæˆ¿', 'æ›¸æˆ¿']
      };
      setMessages(prev => [...prev, reply]);
      return;
    }

    // 5. Normal Chat (Consultant Mode)
    // Add AI Placeholder
    const aiMsgId = Date.now().toString() + 'ai';
    const aiPlaceholder: Message = {
      id: aiMsgId,
      type: 'text',
      content: '...',
      sender: 'ai',
      timestamp: Date.now()
    };
    setMessages(prev => [...prev, aiPlaceholder]);

    try {
      let fullContent = '';
      const apiMessages = messages.map(m => ({
        role: m.sender === 'user' ? 'user' as const : 'assistant' as const,
        content: m.content
      }));
      // Add current user message
      apiMessages.push({ role: 'user', content: text });

      // If we have a pending image analysis (vision summary), pass it
      // Note: We don't store visionSummary in state heavily, usually rely on chat history context
      // But for the immediate turn after upload, it's passed via `visionSummary` prop.
      // Here we assume vision summary is already part of the conversation context if it was outputted by AI previously.
      
      await (async () => {
        for await (const chunk of chatWithDeepseekStream({
          mode: 'consultant', // Always consultant now
          text: text,
          messages: apiMessages
        })) {
          fullContent += chunk;
          setMessages(prev => prev.map(m => m.id === aiMsgId ? { ...m, content: fullContent } : m));
        }
      })();
    } catch (error: any) {
      console.error('Chat Error:', error);
      setMessages(prev => prev.map(m => m.id === aiMsgId ? { ...m, content: 'ç³»çµ±ç¹å¿™ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚' } : m));
    }
  };

  const processRenderIntake = (answer: string) => {
    const nextState = { ...renderState };
    let replyContent = '';
    let options: string[] | undefined;

    switch (renderState.step) {
      case 'space':
        nextState.data.space = answer;
        nextState.step = 'style';
        replyContent = 'å¥½å˜…ã€‚ä½ æƒ³è¡Œå’©é¢¨æ ¼ï¼Ÿ';
        options = ['ç¾ä»£ç°¡ç´„', 'åŒ—æ­é¢¨', 'æ—¥ç³»æœ¨èª¿', 'è¼•å¥¢é¢¨', 'å¥¶æ²¹é¢¨'];
        break;
      case 'style':
        nextState.data.style = answer;
        nextState.step = 'color';
        replyContent = 'æ˜ç™½ã€‚è‰²ç³»æ–¹é¢æœ‰ç„¡ç‰¹åˆ¥å–œå¥½ï¼Ÿ';
        options = ['æ·ºæœ¨è‰²', 'æ·±æœ¨è‰²', 'ç™½è‰²ç‚ºä¸»', 'é»‘ç™½ç°', 'æš–ç°è‰²'];
        break;
      case 'color':
        nextState.data.color = answer;
        nextState.step = 'requirements';
        replyContent = 'æ”¶åˆ°ã€‚æœ€å¾Œï¼Œæœ‰ç„¡å’©æ ¸å¿ƒæ«ƒé«”æˆ–ç‰¹åˆ¥è¦æ±‚ï¼Ÿï¼ˆä¾‹å¦‚ï¼šæƒ³è¦åˆ°é ‚è¡£æ«ƒã€Cå­—é‹æ«ƒã€é¿é–‹çª—å°ä½â€¦ï¼‰';
        break;
      case 'requirements':
        nextState.data.requirements = answer;
        nextState.step = 'ready';
        replyContent = 'è³‡æ–™é½Šå…¨ï¼æˆ‘å¯ä»¥å¹«ä½ ç”Ÿæˆæ•ˆæœåœ–å–‡ã€‚è«‹ç¢ºèªæ˜¯å¦é–‹å§‹ï¼Ÿ';
        options = ['ç”Ÿæˆæ•ˆæœåœ–'];
        break;
    }

    setRenderState(nextState);
    const reply: Message = {
      id: Date.now().toString(),
      type: 'text',
      content: replyContent,
      sender: 'ai',
      timestamp: Date.now(),
      options
    };
    setMessages(prev => [...prev, reply]);
  };

  // Handle Option Click
  const handleOptionClick = (opt: string) => {
    if (opt === 'ç”Ÿæˆæ•ˆæœåœ–') {
      // Trigger Generation
      setGenerating(true);
      const userMsg: Message = {
        id: Date.now().toString(),
        type: 'text',
        content: opt,
        sender: 'user',
        timestamp: Date.now()
      };
      setMessages(prev => [...prev, userMsg]);
      
      triggerImageGeneration(renderState.data, renderState.baseImageBlobUrl, undefined);
    } else {
      handleSendMessage(opt);
    }
  };

  // Trigger Image Generation
  const triggerImageGeneration = async (
      data: any, 
      baseBlobUrl?: string, 
      lastGeneratedUrl?: string, 
      revisionPrompt?: string
  ) => {
    if (!baseBlobUrl && !lastGeneratedUrl) {
        setMessages(prev => [...prev, {
            id: Date.now().toString(),
            type: 'text',
            content: 'ç³»çµ±æµå””åˆ°åº•åœ–ï¼Œè«‹é‡æ–°ä¸Šå‚³ç›¸ç‰‡ã€‚',
            sender: 'ai',
            timestamp: Date.now()
        }]);
        setGenerating(false);
        return;
    }

    const aiMsgId = Date.now().toString() + 'gen';
    setMessages(prev => [...prev, {
        id: aiMsgId,
        type: 'text',
        content: 'æ”¶åˆ°ï¼Œæˆ‘ä¾å®¶å¹«ä½ è¨­è¨ˆç·Šå¼µæ•ˆæœåœ–ï¼Œè«‹ç¨ç­‰â€¦ ğŸ¨',
        sender: 'ai',
        timestamp: Date.now()
    }]);

    try {
        // Construct Prompt
        const space = data.space || 'interior';
        const style = data.style || 'modern';
        const color = data.color || 'light';
        const reqs = data.requirements || '';
        const revision = revisionPrompt ? ` Modification: ${revisionPrompt}` : '';
        
        const prompt = `Realistic interior design render of ${space}, ${style} style, ${color} color scheme. ${reqs}. ${revision}. Keep structural elements unchanged. High quality, photorealistic.`;

        // Use last generated image as base if available (img2img loop), else use original photo
        const sourceUrl = lastGeneratedUrl || baseBlobUrl;

        if (!sourceUrl) throw new Error("No source image URL");

        const res = await generateDesignImage({
            prompt,
            baseImageBlobUrl: sourceUrl,
            size: '1024x1024'
        });

        if (res.ok && (res.resultBlobUrl || res.b64_json)) {
            const resultUrl = res.resultBlobUrl || (res.b64_json ? `data:image/jpeg;base64,${res.b64_json}` : null);
            
            if (resultUrl) {
                // Remove placeholder
                setMessages(prev => prev.filter(m => m.id !== aiMsgId));
                
                // Add Image Message
                const imgMsg: Message = {
                    id: Date.now().toString(),
                    type: 'image',
                    content: resultUrl,
                    sender: 'ai',
                    timestamp: Date.now()
                };
                setMessages(prev => [...prev, imgMsg]);

                // Add Follow-up Text
                const followUp: Message = {
                    id: Date.now().toString() + 'fu',
                    type: 'text',
                    content: 'å‘¢å€‹è¨­è¨ˆä½ è¦ºå¾—é»ï¼Ÿå¦‚æœæƒ³å¾®èª¿ï¼ˆä¾‹å¦‚è½‰è‰²ã€æ”¹æ«ƒæ¬¾ï¼‰ï¼Œå¯ä»¥ç›´æ¥åŒæˆ‘è¬›ã€Œå†æ”¹...ã€ã€‚ğŸ˜Š',
                    sender: 'ai',
                    timestamp: Date.now()
                };
                setMessages(prev => [...prev, followUp]);

                // Update State for next loop
                setRenderState(prev => ({
                    ...prev,
                    lastGeneratedImageUrl: resultUrl,
                    // keep baseImageBlobUrl as original
                }));
            } else {
                throw new Error("No image URL returned");
            }
        } else {
            throw new Error(res.message || "Generation failed");
        }

    } catch (error: any) {
        console.error('Generation Error:', error);
        setMessages(prev => prev.map(m => m.id === aiMsgId ? { 
            ...m, 
            content: `å‡ºåœ–é‡åˆ°å•é¡Œï¼š${error.message || 'è«‹ç¨å¾Œå†è©¦'}ã€‚`,
            options: ['é‡è©¦ç”Ÿæˆ']
        } : m));
    } finally {
        setGenerating(false);
    }
  };

  // Handle Image Upload
  const handleSendImage = (file: File) => {
    // 1. Compress
    compressImage(file, 1536, 0.8).then(blob => {
        const compressedFile = new File([blob], file.name, { type: 'image/jpeg' });
        const reader = new FileReader();
        reader.onload = async (e) => {
            const dataUrl = e.target?.result as string;
            
            // Add User Image Msg
            const msgId = Date.now().toString();
            setMessages(prev => [...prev, {
                id: msgId,
                type: 'image',
                content: dataUrl,
                sender: 'user',
                timestamp: Date.now()
            }]);

            // Add AI Placeholder
            const aiMsgId = Date.now().toString() + 'ai';
            setMessages(prev => [...prev, {
                id: aiMsgId,
                type: 'text',
                content: 'æ”¶åˆ°ç›¸ç‰‡ï¼Œæ­£åœ¨åˆ†æç©ºé–“çµæ§‹â€¦ ğŸ”',
                sender: 'ai',
                timestamp: Date.now()
            }]);

            // Upload to Blob (Background)
            let blobUrl = '';
            try {
                const uploadRes = await uploadImage(compressedFile);
                if (uploadRes?.url) {
                    blobUrl = uploadRes.url;
                    // Update state if we need this for later rendering
                    setRenderState(prev => ({ ...prev, baseImageBlobUrl: blobUrl }));
                }
            } catch (err) {
                console.error('Upload failed:', err);
            }

            // Vision Analysis
            try {
                const visionRes = await analyzeImage({
                    imageDataUrl: dataUrl,
                    imageUrl: blobUrl || undefined,
                    mode: 'consultant' // Use generic mode
                });

                if (visionRes.ok && visionRes.vision_summary) {
                    // Update Placeholder with Analysis
                    setMessages(prev => prev.map(m => m.id === aiMsgId ? {
                        ...m,
                        content: `ã€ç©ºé–“åˆ†æã€‘\n${visionRes.vision_summary}\n\nğŸ’¡ æˆ‘å»ºè­°å¯ä»¥å’æ¨£è¨­è¨ˆï¼š\n(æ­£åœ¨ç”Ÿæˆå»ºè­°...)`
                    } : m));

                    // Generate Advice via Chat API
                    let fullAdvice = '';
                    const adviceMsgId = Date.now().toString() + 'adv';
                    setMessages(prev => [...prev, {
                        id: adviceMsgId,
                        type: 'text',
                        content: '...',
                        sender: 'ai',
                        timestamp: Date.now()
                    }]);

                    for await (const chunk of chatWithDeepseekStream({
                        mode: 'consultant',
                        text: `ç”¨æˆ¶ä¸Šå‚³äº†åœ–ç‰‡ã€‚è¦–è¦ºåˆ†æçµæœï¼š${visionRes.vision_summary}ã€‚è«‹æ ¹æ“šæ­¤åˆ†æï¼Œæä¾›3-4å€‹é‡å°é¦™æ¸¯ç´°å–®ä½çš„å…·é«”å…¨å±‹è¨‚é€ /æ”¶ç´å»ºè­°ã€‚`,
                        messages: [],
                        visionSummary: visionRes.vision_summary
                    })) {
                        fullAdvice += chunk;
                        setMessages(prev => prev.map(m => m.id === adviceMsgId ? { ...m, content: fullAdvice } : m));
                    }
                    
                    // Add "Render" Prompt
                    setTimeout(() => {
                        setMessages(prev => [...prev, {
                            id: Date.now().toString() + 'p',
                            type: 'text',
                            content: 'å¦‚æœä½ æƒ³ç‡ä¸‹å¯¦éš›æ•ˆæœï¼Œå¯ä»¥åŒæˆ‘è¬›ã€Œæƒ³å‡ºåœ–ã€ï¼Œæˆ‘å¹«ä½ ç”Ÿæˆæ•ˆæœåœ–ï¼âœ¨',
                            sender: 'ai',
                            timestamp: Date.now(),
                            options: ['æƒ³è¦æ•ˆæœåœ–']
                        }]);
                    }, 1000);

                } else {
                    throw new Error(visionRes.message || 'ç„¡æ³•è­˜åˆ¥åœ–ç‰‡');
                }
            } catch (error: any) {
                setMessages(prev => prev.map(m => m.id === aiMsgId ? {
                    ...m,
                    content: `åœ–ç‰‡åˆ†æå¤±æ•—ï¼š${error.message}ã€‚è«‹è©¦ä¸‹é‡å‚³æ¸…æ™°å•²å˜…ç›¸ã€‚`
                } : m));
            }
        };
        reader.readAsDataURL(blob);
    });
  };

  return (
    <div className="flex flex-col h-[100dvh] bg-[var(--wa-bg)] overflow-hidden">
      <Header />
      <div className="flex-1 overflow-y-auto px-4 py-1 chat-bg-container relative">
        <div className="flex flex-col gap-1 pb-2 relative z-10">
            {messages.map((msg) => (
                <MessageBubble key={msg.id} message={msg} onOptionClick={handleOptionClick} />
            ))}
            <div ref={chatEndRef} />
        </div>
      </div>
      <InputBar onSendMessage={handleSendMessage} onSendImage={handleSendImage} />
    </div>
  );
};

export default App;
