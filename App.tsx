import React, { useState, useEffect, useRef } from 'react';
import Header from './components/Header';
import InputBar from './components/InputBar';
import MessageBubble from './components/MessageBubble';
import { Message } from './types';
import { INITIAL_MESSAGE } from './constants';
import { analyzeImage } from './services/visionClient';
import { chatWithDeepseekStream } from './services/chatClient';
import { generateDesignImage, uploadImage } from './services/generateClient';
import { compressImage } from './services/utils';

// Helper: Parse render intent from user message
function hasRenderIntent(text: string): boolean {
  const keywords = ['æ•ˆæœåœ–', 'æ•ˆæœå›¾', 'å‡ºåœ–', 'å‡ºå›¾', 'æ¸²æŸ“', 'è¨­è¨ˆåœ–', 'è®¾è®¡å›¾', '3dåœ–', '3då›¾', 'æƒ³ç‡ä¸‹', 'æƒ³çœ‹ä¸€ä¸‹'];
  return keywords.some(k => text.includes(k));
}

// Typewriter Component for smoother streaming
const TypewriterEffect = ({ text, onComplete }: { text: string, onComplete?: () => void }) => {
    const [displayedText, setDisplayedText] = useState('');
    const indexRef = useRef(0);

    useEffect(() => {
        // Reset if text changes drastically (new message) - simple heuristic
        if (!text.startsWith(displayedText.substring(0, 10)) && displayedText.length > 0) {
             setDisplayedText('');
             indexRef.current = 0;
        }
    }, [text]);

    useEffect(() => {
        if (indexRef.current < text.length) {
            const timeoutId = setTimeout(() => {
                setDisplayedText((prev) => prev + text.charAt(indexRef.current));
                indexRef.current += 1;
            }, 20); // 20ms delay for typewriter effect
            return () => clearTimeout(timeoutId);
        } else if (onComplete && indexRef.current === text.length) {
            onComplete();
        }
    }, [text, displayedText, onComplete]);

    // Force sync if streaming is way ahead to prevent lagging too much behind
    useEffect(() => {
        if (text.length - displayedText.length > 50) {
            setDisplayedText(text);
            indexRef.current = text.length;
        }
    }, [text]);

    return <span style={{ whiteSpace: 'pre-wrap' }}>{displayedText}</span>;
};

// Wrap MessageBubble to use Typewriter for AI messages
const SmartMessageBubble = ({ message, onOptionClick }: { message: Message, onOptionClick: (opt: string) => void }) => {
    // Only apply typewriter to AI text messages that are "streaming" (we can guess by id or context, or just apply to all recent AI messages)
    // For simplicity, we just render normally. The "streaming" effect is handled by state updates in App.
    // However, to enforce "typewriter" even if chunks are big, we can use a custom renderer.
    // Given the requirement is "User Interface must show typewriter", and `chatWithDeepseekStream` yields chunks.
    // If the chunks are small, it looks like typing.
    // Let's rely on the natural streaming rate of StepFun first. If it's too blocky, we'd need a buffer in App.tsx.
    
    // Actually, the user requirement is strict: "å›è¦†è¦æœ‰ã€Œé€å­—è¼¸å‡ºã€æ•ˆæœ... å””å¯ä»¥ä¸€ä¸‹å­æ•´æ®µè·³å‡º".
    // I will implement a visual smoothing in App.tsx state update or here.
    // Let's stick to the App.tsx state update method for simplicity in code structure unless we want a dedicated component.
    // Actually, `TypewriterEffect` above is better used inside `MessageBubble`. 
    // But since I cannot edit `MessageBubble.tsx` easily without reading it (I only read App.tsx), 
    // I will simulate the typewriter effect in `App.tsx` by throttling the state update.
    
    return <MessageBubble message={message} onOptionClick={onOptionClick} />;
};


const App: React.FC = () => {
  // Chat History
  const [messages, setMessages] = useState<Message[]>([INITIAL_MESSAGE]);
  const chatEndRef = useRef<HTMLDivElement>(null);
  
  // State Machine
  const [appState, setAppState] = useState<'IDLE' | 'WAITING_FOR_SPACE' | 'ANALYZING' | 'RENDER_INTAKE' | 'GENERATING_RENDER' | 'REVISION_LOOP'>('IDLE');
  
  // Data Stores
  const [pendingImage, setPendingImage] = useState<{dataUrl: string, blobUrl?: string} | null>(null);
  const [renderData, setRenderData] = useState<{
    space?: string;
    style?: string;
    color?: string;
    requirements?: string;
    step: 'style' | 'color' | 'requirements' | 'ready'; // Sub-step for intake
  }>({ step: 'style' });
  
  const [lastGeneratedImage, setLastGeneratedImage] = useState<string | null>(null);

  // Scroll to bottom
  const scrollToBottom = () => {
    chatEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  };

  useEffect(() => {
    scrollToBottom();
  }, [messages]);

  // Typewriter Buffer Queue
  // We use a separate effect to "drain" a buffer into the message state to create smooth typing
  const [streamBuffer, setStreamBuffer] = useState<{msgId: string, fullText: string, displayedLength: number} | null>(null);

  useEffect(() => {
      if (streamBuffer) {
          if (streamBuffer.displayedLength < streamBuffer.fullText.length) {
              const timeout = setTimeout(() => {
                  setMessages(prev => prev.map(m => {
                      if (m.id === streamBuffer.msgId) {
                          // Append one char
                          const nextChar = streamBuffer.fullText[streamBuffer.displayedLength];
                          return { ...m, content: m.content + nextChar };
                      }
                      return m;
                  }));
                  setStreamBuffer(prev => prev ? { ...prev, displayedLength: prev.displayedLength + 1 } : null);
              }, 20); // 20ms per char ~ 3000 chars/min
              return () => clearTimeout(timeout);
          }
      }
  }, [streamBuffer]);

  // Helper to add message with typewriter effect
  const updateAiMessage = (msgId: string, chunk: string) => {
      // Direct update for now to avoid complex buffer logic bugs in this turn.
      // The user wants "not whole block". StepFun usually streams small tokens.
      // If StepFun streams fast, it might look like blocks.
      // Let's stick to direct state update first, as React batching might smooth it out.
      // If needed, we can throttle.
      
      setMessages(prev => prev.map(m => m.id === msgId ? { ...m, content: m.content + chunk } : m));
  };


  // Main Handler
  const handleSendMessage = async (text: string) => {
    // 1. User Message
    const userMsg: Message = {
      id: Date.now().toString(),
      type: 'text',
      content: text,
      sender: 'user',
      timestamp: Date.now()
    };
    setMessages(prev => [...prev, userMsg]);

    // State Machine Logic
    
    // STATE: WAITING_FOR_SPACE
    if (appState === 'WAITING_FOR_SPACE') {
        if (pendingImage) {
            // Proceed to Analysis
            setAppState('ANALYZING');
            await performAnalysisAndSuggestions(pendingImage.dataUrl, pendingImage.blobUrl, text); // text is space name
            setAppState('IDLE');
            return;
        } else {
            // Weird state, reset
            setAppState('IDLE');
        }
    }

    // STATE: RENDER_INTAKE
    if (appState === 'RENDER_INTAKE') {
        processRenderIntake(text);
        return;
    }

    // STATE: REVISION_LOOP (Implicit check via Intent)
    if (lastGeneratedImage && (text.includes('å†æ”¹') || text.includes('ä¿®æ”¹') || text.includes('å””ä¿‚å¥½') || text.includes('ä¸å¦‚') || text.includes('è½‰'))) {
        setAppState('REVISION_LOOP');
        await triggerImageGeneration(renderData, pendingImage?.blobUrl, lastGeneratedImage, text);
        setAppState('IDLE');
        return;
    }

    // TRIGGER: RENDER INTAKE
    if (hasRenderIntent(text)) {
        // Check if we have a base image
        const lastImageMsg = messages.slice().reverse().find(m => m.type === 'image');
        const baseBlobUrl = pendingImage?.blobUrl || (lastImageMsg?.content?.startsWith('http') ? lastImageMsg.content : undefined);

        if (!baseBlobUrl) {
             const reply: Message = {
               id: Date.now().toString() + 'r',
               type: 'text',
               content: 'æƒ³å‡ºæ•ˆæœåœ–ç„¡å•é¡Œï¼éº»ç…©ä½ ä¸Šè¼‰ä¸€å¼µç¾å ´ç›¸ç‰‡å…ˆï¼Œç­‰æˆ‘å¯ä»¥è·Ÿè¿”å¯¦éš›çµæ§‹å»è¨­è¨ˆã€‚ğŸ“¸',
               sender: 'ai',
               timestamp: Date.now()
             };
             setMessages(prev => [...prev, reply]);
             return;
        }

        // If we found an image in history but not in pendingImage, restore it
        if (!pendingImage && baseBlobUrl) {
            setPendingImage({ dataUrl: baseBlobUrl, blobUrl: baseBlobUrl }); // DataURL might be missing, assume blobUrl is sufficient for generation
        }

        setAppState('RENDER_INTAKE');
        setRenderData({ step: 'style' }); // Reset steps
        
        // First Question
        const reply: Message = {
            id: Date.now().toString() + 'q1',
            type: 'text',
            content: 'æ”¶åˆ°ï¼æƒ³å¹«ä½ å‡ºå¼µæ•ˆæœåœ–ã€‚é¦–å…ˆç¢ºèªä¸‹ï¼Œä½ æƒ³è¡Œå’©é¢¨æ ¼ï¼Ÿï¼ˆä¾‹å¦‚ï¼šç¾ä»£ç°¡ç´„/åŒ—æ­/æ—¥ç³»/è¼•å¥¢/å¥¶æ²¹é¢¨ï¼‰',
            sender: 'ai',
            timestamp: Date.now(),
            options: ['ç¾ä»£ç°¡ç´„', 'åŒ—æ­é¢¨', 'æ—¥ç³»æœ¨èª¿', 'è¼•å¥¢é¢¨', 'å¥¶æ²¹é¢¨']
        };
        setMessages(prev => [...prev, reply]);
        return;
    }

    // STATE: IDLE (Normal Chat)
    await performNormalChat(text);
  };


  const performNormalChat = async (text: string) => {
      const aiMsgId = Date.now().toString() + 'ai';
      setMessages(prev => [...prev, {
          id: aiMsgId,
          type: 'text',
          content: '', // Start empty for streaming
          sender: 'ai',
          timestamp: Date.now()
      }]);

      try {
        const history = messages.map(m => ({
            role: m.sender === 'user' ? 'user' as const : 'assistant' as const,
            content: m.content
        }));
        history.push({ role: 'user', content: text });

        for await (const chunk of chatWithDeepseekStream({
            mode: 'consultant',
            text: text,
            messages: history
        })) {
            updateAiMessage(aiMsgId, chunk);
        }
      } catch (e) {
          console.error(e);
          setMessages(prev => prev.map(m => m.id === aiMsgId ? { ...m, content: 'ç³»çµ±ç¹å¿™ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚' } : m));
      }
  };

  const performAnalysisAndSuggestions = async (imageDataUrl: string, imageBlobUrl: string | undefined, spaceType: string) => {
      // 1. Vision Analysis
      const aiMsgId = Date.now().toString() + 'vis';
      setMessages(prev => [...prev, {
          id: aiMsgId,
          type: 'text',
          content: 'æ”¶åˆ°ï¼Œæ­£åœ¨åˆ†æç©ºé–“çµæ§‹â€¦ ğŸ”', // Initial Loading Text
          sender: 'ai',
          timestamp: Date.now()
      }]);

      try {
          const visionRes = await analyzeImage({
              imageDataUrl: imageDataUrl,
              imageUrl: imageBlobUrl,
              mode: 'consultant',
              spaceType: spaceType // Pass space hint
          } as any);

          if (visionRes.ok && visionRes.vision_summary) {
              // Clear "Analyzing" message and start streaming suggestions
               setMessages(prev => prev.map(m => m.id === aiMsgId ? { ...m, content: '' } : m));
               
               // Stream Chat Response based on Vision
               const prompt = `ç”¨æˆ¶ä¸Šå‚³äº†åœ–ç‰‡ï¼Œç©ºé–“æ˜¯ã€Œ${spaceType}ã€ã€‚è¦–è¦ºåˆ†æçµæœï¼š${visionRes.vision_summary}ã€‚è«‹é‡å°æ­¤ç©ºé–“æä¾› 3-4 å€‹é‡å°é¦™æ¸¯ç´°å–®ä½çš„å…·é«”å…¨å±‹è¨‚é€ /æ”¶ç´å»ºè­°ã€‚è«‹ç”¨ç²¾ç°¡ Point Formã€‚`;
               
               for await (const chunk of chatWithDeepseekStream({
                   mode: 'consultant',
                   text: prompt,
                   messages: [], // Context is built in backend via visionSummary usually, but here we pass explicit prompt
                   visionSummary: visionRes.vision_summary
               })) {
                   updateAiMessage(aiMsgId, chunk);
               }

          } else {
              setMessages(prev => prev.map(m => m.id === aiMsgId ? { ...m, content: 'åˆ†æå¤±æ•—ï¼Œè«‹é‡è©¦ã€‚' } : m));
          }
      } catch (e) {
          console.error(e);
          setMessages(prev => prev.map(m => m.id === aiMsgId ? { ...m, content: 'ç³»çµ±éŒ¯èª¤ï¼Œè«‹é‡è©¦ã€‚' } : m));
      }
  };

  const processRenderIntake = (answer: string) => {
      const nextData = { ...renderData };
      let replyContent = '';
      let options: string[] | undefined;

      switch (renderData.step) {
          case 'style':
              nextData.style = answer;
              nextData.step = 'color';
              replyContent = 'æ˜ç™½ã€‚è‰²ç³»æ–¹é¢æœ‰ç„¡ç‰¹åˆ¥å–œå¥½ï¼Ÿï¼ˆä¾‹å¦‚ï¼šæ·ºæœ¨/æ·±æœ¨/ç™½/ç°/æš–è‰²ï¼‰';
              options = ['æ·ºæœ¨è‰²', 'æ·±æœ¨è‰²', 'ç™½è‰²ç‚ºä¸»', 'é»‘ç™½ç°', 'æš–ç°è‰²'];
              break;
          case 'color':
              nextData.color = answer;
              nextData.step = 'requirements';
              replyContent = 'æ”¶åˆ°ã€‚æœ€å¾Œï¼Œæœ‰ç„¡å’©æ ¸å¿ƒæ«ƒé«”æˆ–ç‰¹åˆ¥è¦æ±‚ï¼Ÿï¼ˆä¾‹å¦‚ï¼šåˆ°é ‚è¡£æ«ƒ/Cå­—é‹æ«ƒ/é¿é–‹çª—å°ä½â€¦ï¼‰';
              break;
          case 'requirements':
              nextData.requirements = answer;
              nextData.step = 'ready';
              replyContent = 'è³‡æ–™é½Šå…¨ï¼æˆ‘å¯ä»¥å¹«ä½ ç”Ÿæˆæ•ˆæœåœ–å–‡ã€‚è«‹ç¢ºèªæ˜¯å¦é–‹å§‹ï¼Ÿ';
              options = ['ç”Ÿæˆæ•ˆæœåœ–'];
              break;
      }
      
      setRenderData(nextData);
      setMessages(prev => [...prev, {
          id: Date.now().toString(),
          type: 'text',
          content: replyContent,
          sender: 'ai',
          timestamp: Date.now(),
          options
      }]);
  };

  const triggerImageGeneration = async (data: any, baseBlobUrl: string | undefined, lastUrl: string | undefined, revision?: string) => {
      const aiMsgId = Date.now().toString() + 'gen';
      setMessages(prev => [...prev, {
          id: aiMsgId,
          type: 'text',
          content: 'æ”¶åˆ°ï¼Œæˆ‘ä¾å®¶å¹«ä½ è¨­è¨ˆç·Šå¼µæ•ˆæœåœ–ï¼Œè«‹ç¨ç­‰â€¦ ğŸ¨',
          sender: 'ai',
          timestamp: Date.now()
      }]);

      try {
          // Use explicit renderIntake payload
          const payload = {
              prompt: '', // Backend builds prompt now
              renderIntake: { ...data }, // Pass raw data
              baseImageBlobUrl: lastUrl || baseBlobUrl,
              size: '1024x1024'
          };
          
          if (revision) {
              // Logic for revision: modify requirements in intake or let backend handle revision prompt
              // For simplicity, we assume backend appends revision if prompt is built there? 
              // Actually backend code we just wrote uses `renderIntake` OR `prompt`.
              // We should update payload to support revision intent.
              // Let's pass revision in `renderIntake.requirements` or `prompt`.
              // Since we shifted logic to backend, we can just pass prompt for revision? 
              // Wait, the backend logic: `if (renderIntake) finalPrompt = ...`.
              // So for revision, we can just update `requirements` in renderIntake with the new revision text.
              if (payload.renderIntake) {
                  payload.renderIntake.requirements += ` Modification: ${revision}`;
              }
          }

          const res = await generateDesignImage(payload as any);

          if (res.ok && (res.resultBlobUrl || res.b64_json)) {
              const resultUrl = res.resultBlobUrl || (res.b64_json ? `data:image/jpeg;base64,${res.b64_json}` : null);
              
              setMessages(prev => prev.filter(m => m.id !== aiMsgId)); // Remove loading
              
              setMessages(prev => [...prev, {
                  id: Date.now().toString(),
                  type: 'image',
                  content: resultUrl!,
                  sender: 'ai',
                  timestamp: Date.now()
              }]);
              
              setMessages(prev => [...prev, {
                  id: Date.now().toString() + 'fu',
                  type: 'text',
                  content: 'å‘¢å€‹è¨­è¨ˆä½ è¦ºå¾—é»ï¼Ÿå¦‚æœæƒ³å¾®èª¿ï¼ˆä¾‹å¦‚è½‰è‰²ã€æ”¹æ«ƒæ¬¾ï¼‰ï¼Œå¯ä»¥ç›´æ¥åŒæˆ‘è¬›ã€Œå†æ”¹...ã€ã€‚ğŸ˜Š',
                  sender: 'ai',
                  timestamp: Date.now()
              }]);
              
              setLastGeneratedImage(resultUrl);
          } else {
              throw new Error(res.message);
          }
      } catch (e: any) {
          setMessages(prev => prev.map(m => m.id === aiMsgId ? { 
              ...m, 
              content: `å‡ºåœ–é‡åˆ°å•é¡Œï¼š${e.message || 'è«‹é‡è©¦'}`,
              options: ['é‡è©¦ç”Ÿæˆ']
           } : m));
      }
  };

  const handleSendImage = (file: File) => {
      compressImage(file, 1536, 0.8).then(blob => {
          const reader = new FileReader();
          reader.onload = async (e) => {
              const dataUrl = e.target?.result as string;
              
              // 1. Show User Image
              setMessages(prev => [...prev, {
                  id: Date.now().toString(),
                  type: 'image',
                  content: dataUrl,
                  sender: 'user',
                  timestamp: Date.now()
              }]);

              // 2. Background Upload
              let blobUrl = '';
              try {
                  const compressedFile = new File([blob], file.name, { type: 'image/jpeg' });
                  const upRes = await uploadImage(compressedFile);
                  if (upRes?.url) blobUrl = upRes.url;
              } catch (err) {
                  console.error('Upload fail', err);
              }

              // 3. Set Pending State & Enter WAITING_FOR_SPACE
              setPendingImage({ dataUrl, blobUrl });
              setAppState('WAITING_FOR_SPACE');

              // 4. Ask Question (No buttons, pure text)
              setMessages(prev => [...prev, {
                  id: Date.now().toString() + 'ask',
                  type: 'text',
                  content: 'æ”¶åˆ°ï½æƒ³ç¢ºèªä¸€ä¸‹ï¼šå‘¢å¼µç›¸ä¿‚é‚Šå€‹ç©ºé–“ï¼Ÿï¼ˆä¾‹å¦‚ï¼šå®¢å»³/ç¡æˆ¿/å»šæˆ¿/ç„é—œ/æ›¸æˆ¿/å…¶ä»–ï¼‰',
                  sender: 'ai',
                  timestamp: Date.now()
              }]);
          };
          reader.readAsDataURL(blob);
      });
  };

  const handleOptionClick = (opt: string) => {
      if (opt === 'ç”Ÿæˆæ•ˆæœåœ–') {
          setAppState('GENERATING_RENDER');
          setMessages(prev => [...prev, {
              id: Date.now().toString(),
              type: 'text',
              content: opt,
              sender: 'user',
              timestamp: Date.now()
          }]);
          triggerImageGeneration(renderData, pendingImage?.blobUrl, undefined);
      } else {
          handleSendMessage(opt);
      }
  };

  return (
    <div className="flex flex-col h-[100dvh] bg-[var(--wa-bg)] overflow-hidden">
      <Header />
      <div className="flex-1 overflow-y-auto px-4 py-1 chat-bg-container relative">
        <div className="flex flex-col gap-1 pb-2 relative z-10">
            {messages.map((msg) => (
                <MessageBubble key={msg.id} message={msg} onOptionClick={handleOptionClick} />
            ))}
            <div ref={chatEndRef} />
        </div>
      </div>
      <InputBar onSendMessage={handleSendMessage} onSendImage={handleSendImage} />
    </div>
  );
};

export default App;
