import React, { useState, useEffect, useRef } from 'react';
import { AppShell } from './components/AppShell';
import { AppBar } from './components/AppBar';
import { PhotoCard } from './components/PhotoCard';
import { NextStepCard } from './components/NextStepCard';
import { SummaryCard } from './components/SummaryCard';
import { MessageCard } from './components/MessageCard';
import { RenderResultCard } from './components/RenderResultCard';
import { Composer } from './components/Composer';
import { Message } from './types';
import { INITIAL_MESSAGE } from './constants';
import { analyzeImage } from './services/visionClient';
import { chatWithDeepseekStream } from './services/chatClient';
import { generateDesignImage, uploadImage } from './services/generateClient';
import { compressImage } from './services/utils';

// Helper: Parse render intent
function hasRenderIntent(text: string): boolean {
  const keywords = ['æ•ˆæžœåœ–', 'æ•ˆæžœå›¾', 'å‡ºåœ–', 'å‡ºå›¾', 'æ¸²æŸ“', 'è¨­è¨ˆåœ–', 'è®¾è®¡å›¾', '3dåœ–', '3då›¾', 'æƒ³ç‡ä¸‹', 'æƒ³çœ‹ä¸€ä¸‹'];
  return keywords.some(k => text.includes(k));
}

const App: React.FC = () => {
  // --- Data & State ---
  const [messages, setMessages] = useState<Message[]>([INITIAL_MESSAGE]);
  const [appState, setAppState] = useState<'IDLE' | 'WAITING_FOR_SPACE' | 'ANALYZING' | 'RENDER_INTAKE' | 'GENERATING_RENDER' | 'REVISION_LOOP'>('IDLE');
  
  const [pendingImage, setPendingImage] = useState<{dataUrl: string, blobUrl?: string} | null>(null);
  const [analysisSummary, setAnalysisSummary] = useState<string | null>(null);
  const [lastGeneratedImage, setLastGeneratedImage] = useState<string | null>(null);
  
  const [renderData, setRenderData] = useState<{
    space?: string;
    style?: string;
    color?: string;
    requirements?: string;
    step: 'style' | 'color' | 'requirements' | 'ready'; 
  }>({ step: 'style' });

  // Refs
  const chatEndRef = useRef<HTMLDivElement>(null);

  // Scroll on message change
  useEffect(() => {
    chatEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  }, [messages]);

  // --- Typewriter Logic ---
  const [typewriterBuffer, setTypewriterBuffer] = useState<{msgId: string, queue: string[]} | null>(null);

  useEffect(() => {
      if (!typewriterBuffer || typewriterBuffer.queue.length === 0) return;
      const timer = setInterval(() => {
          setTypewriterBuffer(prev => {
              if (!prev || prev.queue.length === 0) return null;
              const nextChar = prev.queue[0];
              const remaining = prev.queue.slice(1);
              setMessages(current => current.map(m => m.id === prev.msgId ? { ...m, content: m.content + nextChar } : m));
              return { ...prev, queue: remaining };
          });
      }, 15);
      return () => clearInterval(timer);
  }, [typewriterBuffer]);

  const streamToTypewriter = (msgId: string, chunk: string) => {
      setTypewriterBuffer(prev => {
          const chars = chunk.split('');
          if (prev && prev.msgId === msgId) return { ...prev, queue: [...prev.queue, ...chars] };
          return { msgId, queue: chars };
      });
  };

  // --- Handlers ---
  const handleSendMessage = async (text: string) => {
    const userMsg: Message = { id: Date.now().toString(), type: 'text', content: text, sender: 'user', timestamp: Date.now() };
    setMessages(prev => [...prev, userMsg]);

    // WAITING FOR SPACE
    if (appState === 'WAITING_FOR_SPACE') {
        if (pendingImage) {
            setAppState('ANALYZING');
            await performAnalysisAndSuggestions(pendingImage.dataUrl, pendingImage.blobUrl, text);
            setAppState('IDLE');
            return;
        }
        setAppState('IDLE');
    }

    // RENDER INTAKE
    if (appState === 'RENDER_INTAKE') {
        processRenderIntake(text);
        return;
    }

    // REVISION LOOP
    if (lastGeneratedImage && (text.includes('å†æ”¹') || text.includes('ä¿®æ”¹') || text.includes('å””ä¿‚å¥½') || text.includes('è½‰'))) {
        setAppState('REVISION_LOOP');
        await triggerImageGeneration(renderData, pendingImage?.blobUrl, lastGeneratedImage, text);
        setAppState('IDLE');
        return;
    }

    // TRIGGER RENDER
    if (hasRenderIntent(text)) {
        const lastImageMsg = messages.slice().reverse().find(m => m.type === 'image');
        const baseBlobUrl = pendingImage?.blobUrl || (lastImageMsg?.content?.startsWith('http') ? lastImageMsg.content : undefined);

        if (!baseBlobUrl) {
             addAiMessage('æƒ³å‡ºæ•ˆæžœåœ–ç„¡å•é¡Œï¼éº»ç…©ä½ ä¸Šè¼‰ä¸€å¼µç¾å ´ç›¸ç‰‡å…ˆã€‚ðŸ“¸');
             return;
        }

        if (!pendingImage && baseBlobUrl) setPendingImage({ dataUrl: baseBlobUrl, blobUrl: baseBlobUrl });

        setAppState('RENDER_INTAKE');
        setRenderData({ step: 'style' }); 
        addAiMessage('æ”¶åˆ°ï¼æƒ³å¹«ä½ å‡ºå¼µæ•ˆæžœåœ–ã€‚é¦–å…ˆç¢ºèªä¸‹ï¼Œä½ æƒ³è¡Œå’©é¢¨æ ¼ï¼Ÿ', ['ç¾ä»£ç°¡ç´„', 'åŒ—æ­é¢¨', 'æ—¥ç³»æœ¨èª¿', 'è¼•å¥¢é¢¨', 'å¥¶æ²¹é¢¨']);
        return;
    }

    // NORMAL CHAT
    await performNormalChat(text);
  };

  const addAiMessage = (content: string, options?: string[]) => {
      const msg: Message = { id: Date.now().toString(), type: 'text', content, sender: 'ai', timestamp: Date.now(), options };
      setMessages(prev => [...prev, msg]);
  };

  const performNormalChat = async (text: string) => {
      const aiMsgId = Date.now().toString() + 'ai';
      setMessages(prev => [...prev, { id: aiMsgId, type: 'text', content: '', sender: 'ai', timestamp: Date.now() }]);

      try {
        const history = messages.map(m => ({ role: m.sender === 'user' ? 'user' : 'assistant', content: m.content } as any));
        history.push({ role: 'user', content: text });

        for await (const chunk of chatWithDeepseekStream({ mode: 'consultant', text, messages: history })) {
            streamToTypewriter(aiMsgId, chunk);
        }
      } catch (e) {
          console.error(e);
          setMessages(prev => prev.map(m => m.id === aiMsgId ? { ...m, content: 'ç³»çµ±ç¹å¿™ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚' } : m));
      }
  };

  const performAnalysisAndSuggestions = async (imageDataUrl: string, imageBlobUrl: string | undefined, spaceType: string) => {
      // Show analyzing in chat stream? Or just status card? 
      // User asked for "SummaryCard" with analysis.
      // Let's also stream the suggestions to chat.
      const aiMsgId = Date.now().toString() + 'vis';
      setMessages(prev => [...prev, { id: aiMsgId, type: 'text', content: 'æ”¶åˆ°ï¼Œæ­£åœ¨åˆ†æžç©ºé–“çµæ§‹â€¦ ðŸ”', sender: 'ai', timestamp: Date.now() }]);

      try {
          const visionRes = await analyzeImage({ imageDataUrl, imageUrl: imageBlobUrl, mode: 'consultant', spaceType } as any);

          if (visionRes.ok && visionRes.vision_summary) {
               setAnalysisSummary(visionRes.vision_summary); // Populate Summary Card
               setMessages(prev => prev.map(m => m.id === aiMsgId ? { ...m, content: '' } : m)); // Clear placeholder
               
               const prompt = `ç”¨æˆ¶ä¸Šå‚³äº†åœ–ç‰‡ï¼Œç©ºé–“æ˜¯ã€Œ${spaceType}ã€ã€‚è¦–è¦ºåˆ†æžçµæžœï¼š${visionRes.vision_summary}ã€‚è«‹é‡å°æ­¤ç©ºé–“æä¾› 3-4 å€‹é‡å°é¦™æ¸¯ç´°å–®ä½çš„å…·é«”å…¨å±‹è¨‚é€ /æ”¶ç´å»ºè­°ã€‚è«‹ç”¨ç²¾ç°¡ Point Formã€‚`;
               
               for await (const chunk of chatWithDeepseekStream({ mode: 'consultant', text: prompt, messages: [], visionSummary: visionRes.vision_summary })) {
                   streamToTypewriter(aiMsgId, chunk);
               }
          } else {
              setMessages(prev => prev.map(m => m.id === aiMsgId ? { ...m, content: 'åˆ†æžå¤±æ•—ï¼Œè«‹é‡è©¦ã€‚' } : m));
          }
      } catch (e) {
          console.error(e);
          setMessages(prev => prev.map(m => m.id === aiMsgId ? { ...m, content: 'ç³»çµ±éŒ¯èª¤ï¼Œè«‹é‡è©¦ã€‚' } : m));
      }
  };

  const processRenderIntake = (answer: string) => {
      const nextData = { ...renderData };
      let replyContent = '';
      let options: string[] | undefined;

      switch (renderData.step) {
          case 'style':
              nextData.style = answer;
              nextData.step = 'color';
              replyContent = 'æ˜Žç™½ã€‚è‰²ç³»æ–¹é¢æœ‰ç„¡ç‰¹åˆ¥å–œå¥½ï¼Ÿ';
              options = ['æ·ºæœ¨è‰²', 'æ·±æœ¨è‰²', 'ç™½è‰²ç‚ºä¸»', 'é»‘ç™½ç°', 'æš–ç°è‰²'];
              break;
          case 'color':
              nextData.color = answer;
              nextData.step = 'requirements';
              replyContent = 'æ”¶åˆ°ã€‚æœ€å¾Œï¼Œæœ‰ç„¡å’©æ ¸å¿ƒæ«ƒé«”æˆ–ç‰¹åˆ¥è¦æ±‚ï¼Ÿ';
              break;
          case 'requirements':
              nextData.requirements = answer;
              nextData.step = 'ready';
              replyContent = 'è³‡æ–™é½Šå…¨ï¼æˆ‘å¯ä»¥å¹«ä½ ç”Ÿæˆæ•ˆæžœåœ–å–‡ã€‚è«‹ç¢ºèªæ˜¯å¦é–‹å§‹ï¼Ÿ';
              options = ['ç”Ÿæˆæ•ˆæžœåœ–'];
              break;
      }
      setRenderData(nextData);
      addAiMessage(replyContent, options);
  };

  const triggerImageGeneration = async (data: any, baseBlobUrl: string | undefined, lastUrl: string | undefined, revision?: string) => {
      const aiMsgId = Date.now().toString() + 'gen';
      // Use a temporary message to show loading state
      setMessages(prev => [...prev, { id: aiMsgId, type: 'text', content: 'æ”¶åˆ°ï¼Œæˆ‘ä¾å®¶å¹«ä½ è¨­è¨ˆç·Šå¼µæ•ˆæžœåœ–ï¼Œè«‹ç¨ç­‰â€¦ ðŸŽ¨', sender: 'ai', timestamp: Date.now() }]);

      try {
          const payload = {
              prompt: '', renderIntake: { ...data }, baseImageBlobUrl: lastUrl || baseBlobUrl, size: '1024x1024'
          };
          if (revision && payload.renderIntake) payload.renderIntake.requirements += ` Modification: ${revision}`;

          const res = await generateDesignImage(payload as any);

          if (res.ok && (res.resultBlobUrl || res.b64_json)) {
              const resultUrl = res.resultBlobUrl || (res.b64_json ? `data:image/jpeg;base64,${res.b64_json}` : null);
              
              setMessages(prev => prev.filter(m => m.id !== aiMsgId)); // Remove loading
              
              // Add Special Result Card in Stream (using RenderResultCard component inside message stream logic? 
              // Actually better to push a message that triggers the card, or just push a message with type='image' and handle it customly)
              // Let's use a message type='image' but the MessageCard component will ignore it, and we render RenderResultCard manually?
              // Or better, add a special message type.
              // For now, let's keep it simple: Add message type='render_result' (we need to cast or ignore TS for quick fix)
              
              // We'll use type='image' but with content as the URL. 
              // And we can update `MessageCard` to render `RenderResultCard` if we want, OR just render it in the list.
              // But `MessageCard` currently returns null for image. 
              // We should update `MessageCard` or handle it in App.tsx map.
              
              const resultMsg: Message = {
                  id: Date.now().toString(),
                  type: 'image', // We will intercept this in render loop
                  content: resultUrl!,
                  sender: 'ai',
                  timestamp: Date.now()
              };
              setMessages(prev => [...prev, resultMsg]);
              setLastGeneratedImage(resultUrl!);
          } else {
              throw new Error(res.message);
          }
      } catch (e: any) {
          setMessages(prev => prev.map(m => m.id === aiMsgId ? { ...m, content: `å‡ºåœ–é‡åˆ°å•é¡Œï¼š${e.message || 'è«‹é‡è©¦'}`, options: ['é‡è©¦ç”Ÿæˆ'] } : m));
      }
  };

  const handleSendImage = (file: File) => {
      compressImage(file, 1536, 0.8).then(blob => {
          const reader = new FileReader();
          reader.onload = async (e) => {
              const dataUrl = e.target?.result as string;
              
              // Optimistic UI
              setPendingImage({ dataUrl, blobUrl: '' }); // Blob url comes later
              setAppState('WAITING_FOR_SPACE');
              
              // Upload
              try {
                  const compressedFile = new File([blob], file.name, { type: 'image/jpeg' });
                  const upRes = await uploadImage(compressedFile);
                  if (upRes?.url) setPendingImage(prev => prev ? { ...prev, blobUrl: upRes.url } : null);
              } catch (err) { console.error(err); }
          };
          reader.readAsDataURL(blob);
      });
  };

  const handleOptionClick = (opt: string) => {
      if (opt === 'ç”Ÿæˆæ•ˆæžœåœ–') {
          setAppState('GENERATING_RENDER');
          setMessages(prev => [...prev, { id: Date.now().toString(), type: 'text', content: opt, sender: 'user', timestamp: Date.now() }]);
          triggerImageGeneration(renderData, pendingImage?.blobUrl, undefined);
      } else {
          handleSendMessage(opt);
      }
  };

  // Determine Main Photo Status
  const getPhotoStatus = () => {
      if (appState === 'WAITING_FOR_SPACE') return 'waiting';
      if (appState === 'ANALYZING') return 'analyzing';
      if (appState === 'GENERATING_RENDER') return 'rendering';
      return 'done';
  };

  return (
    <AppShell>
      <AppBar />
      
      <div className="flex-1 overflow-y-auto overflow-x-hidden relative scrollbar-none">
        
        {/* Top Workspace Area */}
        <div className="pt-2 pb-4">
            {/* Show PhotoCard if we have an active image */}
            {(pendingImage || lastGeneratedImage) && (
                <PhotoCard 
                    imageUrl={lastGeneratedImage || pendingImage!.dataUrl} 
                    status={getPhotoStatus()}
                    timestamp={Date.now()}
                    onExpand={() => { /* TODO: Lightbox */ }}
                />
            )}

            {/* Next Step Hint */}
            {appState === 'WAITING_FOR_SPACE' && (
                <NextStepCard text="æ”¶åˆ°ï½žæƒ³ç¢ºèªä¸€ä¸‹ï¼šå‘¢å¼µç›¸ä¿‚é‚Šå€‹ç©ºé–“ï¼Ÿï¼ˆä¾‹å¦‚ï¼šå®¢å»³/ç¡æˆ¿/å»šæˆ¿/çŽ„é—œ/æ›¸æˆ¿/å…¶ä»–ï¼‰" />
            )}

            {/* Analysis Summary */}
            {analysisSummary && <SummaryCard summary={analysisSummary} />}
        </div>

        {/* Conversation Stream */}
        <div className="pb-4">
            {messages.map((msg) => {
                if (msg.type === 'image' && msg.sender === 'ai') {
                    // It's a Render Result
                    return (
                        <RenderResultCard 
                            key={msg.id} 
                            imageUrl={msg.content} 
                            onModify={() => handleSendMessage('æˆ‘æƒ³æ”¹...')} 
                            onWhatsApp={() => window.open('https://wa.me/85212345678', '_blank')} 
                        />
                    );
                }
                // Skip user uploaded image messages in stream if they are shown in PhotoCard? 
                // User requirement: "Conversation (Proposal Card style)..."
                // Let's keep text messages.
                if (msg.type === 'image' && msg.sender === 'user') return null; 

                return <MessageCard key={msg.id} message={msg} onOptionClick={handleOptionClick} />;
            })}
            <div ref={chatEndRef} />
        </div>

      </div>

      <Composer onSendMessage={handleSendMessage} onSendImage={handleSendImage} disabled={appState === 'ANALYZING' || appState === 'GENERATING_RENDER'} />
    </AppShell>
  );
};

export default App;
